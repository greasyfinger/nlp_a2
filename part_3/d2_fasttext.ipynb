{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> fastText Dataset 2</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import TrainingArguments\n",
    "from sklearn.metrics import f1_score\n",
    "import json\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps_device = torch.device(\"mps\")\n",
    "# define helper functions\n",
    "\n",
    "def argmax(vec):\n",
    "\n",
    "    \"\"\" \n",
    "    argmax as the name suggests finds the value \n",
    "    of the argument that gives the maximum value\n",
    "    of a target function\n",
    "    \"\"\"\n",
    "\n",
    "    # return max value of all elements in the input tensor\n",
    "    _, idx = torch.max(vec, 1) \n",
    "    return idx.item()\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = []\n",
    "    for word in seq:\n",
    "        if word not in to_ix.keys():\n",
    "            idxs.append(to_ix['<UNK>'])\n",
    "        else:\n",
    "            idxs.append(to_ix[word])\n",
    "            \n",
    "    return torch.tensor(idxs, dtype=torch.long, device=mps_device)\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "\n",
    "    # view returns a new tensor w/ a different shape\n",
    "    # maintaining the number of elements\n",
    "\n",
    "    # expand simply expands a tensor to a larger size\n",
    "    # say ([1], [2], [3]).expand(3,3) would give\n",
    "    # ([1,1,1],\n",
    "    #  [2,2,2],\n",
    "    #  [3,3,3])\n",
    "\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    \n",
    "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps_device = torch.device(\"mps\")\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    #BILSTM CRF is a subclass inheriting from the (nn.Module) superclass\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim, word_embeds):\n",
    "        super(BiLSTM_CRF, self).__init__() #call the init method of the superclass (nn.Module)\n",
    "\n",
    "        START_TAG = \"<START>\"\n",
    "        STOP_TAG = \"<STOP>\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        # lookup table that stores embeddings\n",
    "        # self.word_embeds = nn.Embedding(vocab_size, embedding_dim).to(mps_device)\n",
    "        self.word_embeds = word_embeds\n",
    "\n",
    "        #define the lstm\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True).to(mps_device)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size).to(mps_device)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size)).to(mps_device)\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2).to(mps_device),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2).to(mps_device))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "\n",
    "        START_TAG = \"<START>\"\n",
    "        STOP_TAG = \"<STOP>\"\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(mps_device)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "\n",
    "        START_TAG = \"<START>\"\n",
    "        STOP_TAG = \"<STOP>\"\n",
    "\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1).to(mps_device)\n",
    "        # tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "\n",
    "        tags_tensor = torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long, device=mps_device)\n",
    "        tags_tensor = torch.cat([tags_tensor, tags.to(mps_device)])\n",
    "        \n",
    "        for i, feat in enumerate(feats):\n",
    "\n",
    "            score = score + self.transitions[tags_tensor[i + 1], tags_tensor[i]] + feat[tags_tensor[i + 1]]\n",
    "\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags_tensor[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "\n",
    "        START_TAG = \"<START>\"\n",
    "        STOP_TAG = \"<STOP>\"\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000., device=mps_device)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "            \n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<UNK>': 0, 'I': 1, 'charge': 2, 'it': 3, 'at': 4, 'night': 5, 'and': 6, 'skip': 7, 'taking': 8, 'the': 9, 'cord': 10, 'with': 11, 'me': 12, 'because': 13, 'of': 14, 'good': 15, 'battery': 16, 'life': 17, '.': 18, 'is': 19, 'high': 20, 'quality': 21, ',': 22, 'has': 23, 'a': 24, 'killer': 25, 'GUI': 26, 'extremely': 27, 'stable': 28, 'highly': 29, 'expandable': 30, 'bundled': 31, 'lots': 32, 'very': 33, 'applications': 34, 'easy': 35, 'to': 36, 'use': 37, 'absolutely': 38, 'gorgeous': 39, 'Easy': 40, 'start': 41, 'up': 42, 'does': 43, 'not': 44, 'overheat': 45, 'as': 46, 'much': 47, 'other': 48, 'laptops': 49, 'Great': 50, 'laptop': 51, 'that': 52, 'offers': 53, 'many': 54, 'great': 55, 'features': 56, '!': 57, 'One': 58, 'turned': 59, 'freaking': 60, 'thing': 61, 'off': 62, 'after': 63, 'using': 64, 'next': 65, 'day': 66, 'turn': 67, 'on': 68, 'no': 69, 'screen': 70, 'all': 71, 'dark': 72, 'power': 73, 'light': 74, 'steady': 75, 'hard': 76, 'drive': 77, 'flashing': 78, 'usually': 79, 'However': 80, 'multi-touch': 81, 'gestures': 82, 'large': 83, 'tracking': 84, 'area': 85, 'make': 86, 'having': 87, 'an': 88, 'external': 89, 'mouse': 90, 'unnecessary': 91, '(': 92, 'unless': 93, 'you': 94, \"'re\": 95, 'gaming': 96, ')': 97, 'love': 98, 'way': 99, 'entire': 100, 'suite': 101, 'software': 102, 'works': 103, 'together': 104, 'The': 105, 'speed': 106, 'incredible': 107, 'am': 108, 'more': 109, 'than': 110, 'satisfied': 111, 'can': 112, 'barely': 113, 'any': 114, 'usb': 115, 'devices': 116, 'they': 117, 'will': 118, 'stay': 119, 'connected': 120, 'properly': 121, 'When': 122, 'finally': 123, 'had': 124, 'everything': 125, 'running': 126, 'my': 127, 'installed': 128, 'plugged': 129, 'in': 130, 'droid': 131, 'recharge': 132, 'system': 133, 'crashed': 134, 'Pairing': 135, 'iPhone': 136, 'pure': 137, 'pleasure': 138, '-': 139, 'talk': 140, 'about': 141, 'painless': 142, 'syncing': 143, 'used': 144, 'take': 145, 'forever': 146, 'now': 147, \"'s\": 148, 'snap': 149, 'also': 150, 'got': 151, 'added': 152, 'bonus': 153, '30': 154, \"''\": 155, 'HD': 156, 'Monitor': 157, 'which': 158, 'really': 159, 'helps': 160, 'extend': 161, 'keep': 162, 'eyes': 163, 'fresh': 164, 'machine': 165, 'slow': 166, 'boot': 167, 'occasionally': 168, 'crashes': 169, 'completely': 170, 'After': 171, 'paying': 172, 'several': 173, 'hundred': 174, 'dollars': 175, 'for': 176, 'this': 177, 'service': 178, 'frustrating': 179, 'get': 180, 'help': 181, 'hours': 182, 'operating': 183, 'preloaded': 184, 'best': 185, 'price': 186, 'along': 187, 'some': 188, 'newer': 189, 'YOU': 190, 'WILL': 191, 'NOT': 192, 'BE': 193, 'ABLE': 194, 'TO': 195, 'TALK': 196, 'AN': 197, 'AMERICAN': 198, 'WARRANTY': 199, 'SERVICE': 200, 'IS': 201, 'OUT': 202, 'OF': 203, 'COUNTRY': 204, 'but': 205, 'i': 206, 'have': 207, 'realized': 208, 'its': 209, 'problem': 210, 'brand': 211, 'Also': 212, 'kinda': 213, 'loud': 214, 'when': 215, 'fan': 216, 'was': 217, 'There': 218, 'seemed': 219, 'be': 220, 'disc': 221, 'certain': 222, 'times': 223, 'windows': 224, 'loads': 225, 'claims': 226, 'able': 227, 'find': 228, 'drivers': 229, 'or': 230, 'files': 231, 'Speaking': 232, 'browser': 233, 'too': 234, 'problems': 235, 'keyboard': 236, 'slick': 237, 'It': 238, 'like': 239, '9': 240, 'punds': 241, 'if': 242, 'look': 243, 'past': 244, 'GREAT': 245, 'just': 246, 'fast': 247, 'one': 248, 'program': 249, 'open': 250, 'sixteen': 251, 'Still': 252, 'under': 253, 'warrenty': 254, 'so': 255, 'called': 256, 'Toshiba': 257, 'Amazing': 258, 'Quality': 259, 'A': 260, 'month': 261, 'ago': 262, 'motherboard': 263, 'died': 264, 'comes': 265, 'work': 266, 'trying': 267, 'fix': 268, 'started': 269, 'working': 270, 'Then': 271, '4': 272, 'months': 273, 'charger': 274, 'stopped': 275, 'forced': 276, 'go': 277, 'out': 278, 'buy': 279, 'new': 280, 'hardware': 281, 'computer': 282, 'If': 283, 'website': 284, 'ever': 285, 'freezes': 286, 'rare': 287, 'force': 288, 'quit': 289, 'enjoy': 290, 'fact': 291, 'MacBook': 292, 'Pro': 293, 'allows': 294, 'run': 295, 'Windows': 296, '7': 297, 'by': 298, 'VMWare': 299, 'wanted': 300, 'purchase': 301, 'extended': 302, 'warranty': 303, 'refused': 304, 'knew': 305, 'trouble': 306, 'We': 307, 'upgraded': 308, 'memory': 309, 'four': 310, 'gigabytes': 311, 'order': 312, 'advantage': 313, 'performace': 314, 'increase': 315, 'reality': 316, 'heated': 317, 'quickly': 318, 'took': 319, 'long': 320, 'do': 321, 'simple': 322, 'things': 323, 'opening': 324, 'Documents': 325, 'folder': 326, 'always': 327, 'PCs': 328, 'been': 329, 'constantly': 330, 'frustrated': 331, 'crashing': 332, 'poorly': 333, 'designed': 334, 'systems': 335, 'were': 336, 'never': 337, 'intuitive': 338, 'within': 339, '5': 340, 'crapped': 341, 'And': 342, 'iphone': 343, 'ipod': 344, 'touch': 345, 'connect': 346, 'download': 347, 'songs': 348, 'glass': 349, 'touchpad': 350, 'continued': 351, 'AGAIN': 352, 'replaced': 353, 'mother': 354, 'board': 355, 'yet': 356, 'again': 357, 'HP': 358, 'sends': 359, 'back': 360, 'screwed': 361, 'Oh': 362, 'yeah': 363, \"n't\": 364, 'forget': 365, 'expensive': 366, 'shipping': 367, 'from': 368, 'Everything': 369, 'Mac': 370, 'simpler': 371, 'Microsoft': 372, 'lot': 373, 'writing': 374, 'editing': 375, 'since': 376, 'there': 377, 'forward': 378, 'delete': 379, 'key': 380, 'Its': 381, 'ease': 382, 'top': 383, 'Apple-': 384, 'their': 385, 'phone': 386, 'assistance': 387, 'bellying': 388, 'genius': 389, 'bar': 390, '--': 391, 'beat': 392, '10': 393, 'hour': 394, 'doing': 395, 'web': 396, 'browsing': 397, 'word': 398, 'making': 399, 'perfect': 400, 'classroom': 401, 'office': 402, 'terms': 403, 'movie': 404, 'playing': 405, \"'ll\": 406, 'over': 407, 'Acer': 408, 'set': 409, 'FREE': 410, 'recovery': 411, 'discs': 412, 'are': 413, 'available': 414, 'asked': 415, 'Enabling': 416, 'timer': 417, 'useless': 418, 'need': 419, 'virus': 420, 'protection': 421, 'saves': 422, 'time': 423, 'money': 424, 'But': 425, 'we': 426, 'paid': 427, 'bluetooth': 428, 'none': 429, 'reliable': 430, 'bugged': 431, 'responds': 432, 'well': 433, 'replace': 434, 'April': 435, 'representive': 436, 'believe': 437, 'robbery': 438, 'charged': 439, 'enormous': 440, 'amounts': 441, 'badly': 442, 'most': 443, 'people': 444, 'would': 445, 'went': 446, 'XP': 447, 'could': 448, 'WIndows': 449, 'vast': 450, 'improvment': 451, 'Vista': 452, 'Dell': 453, 'customer': 454, 'disservice': 455, 'insult': 456, 'customers': 457, 'who': 458, 'pay': 459, 'shoddy': 460, 'products': 461, 'talking': 462, 'knowledgeable': 463, 'sales': 464, 'associate': 465, 'chose': 466, 'white': 467, 'want': 468, 'bang-up': 469, 'Apple': 470, ';': 471, 'You': 472, 'wo': 473, 'spend': 474, 'gobs': 475, 'inefficient': 476, 'needs': 477, 'updated': 478, 'every': 479, 'drains': 480, 'your': 481, 'wallet': 482, 'color': 483, 'even': 484, 'cool': 485, 'keys': 486, 'weird': 487, 'places': 488, 'Yes': 489, 'average': 490, 'comparison': 491, 'style': 492, 'factor': 493, 'mention': 494, 'few': 495, 'slowly': 496, 'less': 497, 'hold': 498, 'until': 499, 'ca': 500, 'leave': 501, 'unplugged': 502, 'minutes': 503, 'without': 504, 'dying': 505, 'BEST': 506, 'BUY': 507, 'STARS': 508, '+': 509, 'respect': 510, 'old': 511, 'men': 512, 'familiar': 513, 'technology': 514, 'DELL': 515, 'COMPUTERS': 516, '3': 517, 'stars': 518, 'SUPPORT': 519, 'owes': 520, 'couple': 521, 'complaints': 522, 'desktop': 523, 'maybe': 524, 'sits': 525, 'carry': 526, 'around': 527, 'jar': 528, 'cost': 529, 'construction': 530, 'longevity': 531, 'PC': 532, 'security': 533, 'minimal': 534, 'arisen': 535, 'worth': 536, 'pricetag': 537, 'gets': 538, 'stuck': 539, 'tapping': 540, 'wish': 541, 'webcam': 542, 'though': 543, 'then': 544, 'Another': 545, 'might': 546, 'add': 547, 'excellent': 548, 'drawback': 549, 'backlit': 550, 'volume': 551, 'louder': 552, 'didnt': 553, 'break': 554, 'only': 555, 'play': 556, 'casual': 557, 'games': 558, 'online': 559, 'responsive': 560, \"'d\": 561, 'hoped': 562, 'feel': 563, 'standpoint': 564, 'somehow': 565, 'bit': 566, 'sturdy': 567, 'dont': 568, 'apples': 569, 'generally': 570, 'safari': 571, 'install': 572, 'Mozzilla': 573, 'firfox': 574, 'single': 575, 'display': 576, 'delivery': 577, 'hesitate': 578, \"'ve\": 579, 'impressed': 580, 'performance': 581, 'such': 582, 'small': 583, 'amount': 584, 'terrific': 585, 'including': 586, 'replacements': 587, 'built-in': 588, 'mic': 589, 'shorting': 590, 'anytime': 591, 'touched': 592, 'lid': 593, 'mind': 594, 'means': 595, 'communication': 596, 'fiance': 597, 'deployed': 598, 'suffered': 599, 'thru': 600, 'constandly': 601, 'reset': 602, 'cam': 603, 'bad': 604, 'connector': 605, 'supply': 606, 'shortly': 607, 'expires': 608, 'issues': 609, 'My': 610, 'dad': 611, 'first': 612, 'Toshibas': 613, 'made': 614, 'yes': 615, 'abit': 616, 'still': 617, 'hooked': 618, 'ethernet': 619, 'Mostly': 620, 'drag': 621, 'drop': 622, 'feature': 623, 'oh': 624, 'fancy': 625, 'breaks': 626, 'guess': 627, 'send': 628, 'fixed': 629, '?': 630, 'ordered': 631, 'through': 632, 'MacMall': 633, 'saved': 634, 'tax': 635, 'incurred': 636, 'buying': 637, 'locally': 638, 'Of': 639, 'course': 640, 'packages': 641, 'came': 642, 'free': 643, 'iWork': 644, 'GarageBand': 645, 'iMovie': 646, 'crystal': 647, 'clear': 648, 'amazing': 649, 'colors': 650, 'resolution': 651, 'little': 652, 'year': 653, 'owning': 654, 'monitor': 655, 'iTunes': 656, 'become': 657, 'ingrained': 658, 'our': 659, 'lexicon': 660, 'Size': 661, ':': 662, 'know': 663, '13': 664, 'especially': 665, 'replacement': 666, 'cares': 667, 'incredibly': 668, 'bright': 669, 'brighter': 670, 'PowerBook': 671, 'crisp': 672, 'menu': 673, 'easiest': 674, 'navigate': 675, 'due': 676, 'stacking': 677, 'Really': 678, 'textured': 679, 'surface': 680, 'shows': 681, 'fingerprints': 682, 'nice': 683, 'awesome': 684, 'iLife': 685, 'Snow': 686, 'Leopard': 687, 'X': 688, 'thought': 689, 'learning': 690, 'OS': 691, 'easily': 692, 'picked': 693, 'lightweight': 694, 'They': 695, 'longer': 696, 'computers': 697, 'friends': 698, 'older': 699, 'PowerBooks': 700, 'check': 701, 'same': 702, 'notebook': 703, 'above': 704, 'missing': 705, 'ports': 706, 'dual': 707, 'core': 708, 'AMD': 709, 'Intel': 710, 'processor': 711, 'This': 712, 'sleek': 713, 'chiclet': 714, 'type': 715, 'bought': 716, 'protector': 717, 'pad': 718, 'magnetic': 719, 'plug-in': 720, 'charging': 721, 'put': 722, 'test': 723, 'accident': 724, 'innovation': 725, 'While': 726, 'saving': 727, 'grace': 728, 'least': 729, 'stand': 730, 'behind': 731, 'support': 732, 'justify': 733, 'premium': 734, 'graphics': 735, 'lock-up': 736, 'myriad': 737, 'causes': 738, 'common': 739, 'being': 740, 'corrupted': 741, 'version': 742, 'Appleworks': 743, 'render': 744, 'paint': 745, 'wears': 746, 'farther': 747, 'usual': 748, 'store': 749, 'honored': 750, 'comment': 751, 'recommend': 752, 'warrentys': 753, 'sent': 754, 'box': 755, 'right': 756, 'away': 757, 'postage': 758, 'whatnot': 759, 'CD': 760, 'reading': 761, 'anything': 762, 'size': 763, 'email': 764, 'On': 765, 'G4': 766, 'trackpad': 767, 'did': 768, 'becomes': 769, 'hot': 770, 'while': 771, 'Yeah': 772, 'smarty': 773, 'pants': 774, '``': 775, 'Software': 776, 'Compared': 777, 'early': 778, '2011': 779, 'edition': 780, 'see': 781, 'inbuilt': 782, 'prompted': 783, 'report': 784, 'promptly': 785, 'body': 786, 'cheaply': 787, 'interesting': 788, 'how': 789, 'holds': 790, 'With': 791, 'mac': 792, 'worry': 793, 'antivirus': 794, 'firewall': 795, 'wonderful': 796, 'Am': 797, 'glad': 798, 'netbook': 799, 'low': 800, 'team': 801, 'assists': 802, 'nicely': 803, 'choosing': 804, 'think': 805, 'part': 806, 'issue': 807, 'latest': 808, 'should': 809, 'waited': 810, 'video': 811, 'chat': 812, 'iffy': 813, 'im': 814, 'sure': 815, 'once': 816, 'unpdate': 817, 'macbook': 818, 'book': 819, 'better': 820, 'That': 821, 'whole': 822, 'experience': 823, 'ridiculous': 824, 'told': 825, 'us': 826, '$': 827, '175': 828, 'portable': 829, 'cheaper': 830, 'Fan': 831, 'vents': 832, 'side': 833, 'cooling': 834, 'needed': 835, 'takes': 836, 'short': 837, 'load': 838, 'viruses': 839, 'Wasted': 840, '8': 841, 'installation': 842, 'far': 843, 'exceeded': 844, 'expectations': 845, 'storage': 846, 'abilitiy': 847, 'enjoying': 848, 'provides': 849, 'Suffice': 850, 'say': 851, 'keeps': 852, 'going': 853, 'blazing': 854, 'user': 855, 'friendly': 856, 'those': 857, 'switch': 858, 'practice': 859, 'full': 860, 'handy': 861, 'music-management': 862, 'essential': 863, 'anyone': 864, 'iPod': 865, 'programs': 866, 'esay': 867, 'quick': 868, 'process': 869, 'charm': 870, 'Tech': 871, 'tells': 872, 'latter': 873, 'offered': 874, 'happens': 875, 'Sells': 876, 'sacrificing': 877, 'SP2': 878, 'caused': 879, 'remove': 880, 'second': 881, 'card': 882, 'unreliable': 883, 'overall': 884, 'returning': 885, 'assuring': 886, 'processing': 887, 'internet': 888, 'users': 889, 'Since': 890, 'smooth': 891, 'meets': 892, 'spec': 893, 'quite': 894, 'Who': 895, 'DVD': 896, 'burner': 897, '80-gigabyte': 898, 'fairly': 899, 'chip': 900, 'As': 901, 'soon': 902, 'discovered': 903, 'reason': 904, 'similarly-configured': 905, 'Sony': 906, 'machines': 907, 'higher-quality': 908, 'components': 909, 'faster': 910, 'better-configured': 911, 'end': 912, 'lasting': 913, 'itself': 914, 'ran': 915, 'smoothly': 916, 'Like': 917, 'operation': 918, 'tarnished': 919, 'heart': 920, 'likely': 921, 'poor': 922, 'grounding': 923, 'isolation': 924, 'between': 925, \"'m\": 926, 'hoping': 927, 'ground': 928, 'loop': 929, 'isolator': 930, 'expected': 931, 'product': 932, 'range': 933, \"'have\": 934, '1': 935, '1/2': 936, 'Once': 937, 'leading': 938, 'edge': 939, 'razor': 940, 'sharp': 941, 'Maximum': 942, 'sound': 943, 'nearly': 944, 'loaded': 945, 'via': 946, 'Bootcamp': 947, 'flawlessly': 948, 'Laptop': 949, 'action': 950, 'pack': 951, 'Although': 952, 'higher': 953, 'Macbooks': 954, 'dough': 955, 'So': 956, 'what': 957, 'laptops/mobile': 958, 'phones': 959, 'chic': 960, 'terrible': 961, 'hate': 962, 'done': 963, 'change': 964, 'acer': 965, 'arcade': 966, 'these': 967, 'reallythe': 968, 'two': 969, 'liked': 970, 'Have': 971, 'leaps': 972, 'bounds': 973, 'ahead': 974, 'opinion': 975, 'gave': 976, 'daughter': 977, 'hated': 978, 'cd': 979, 'listen': 980, 'music': 981, 'schoolwork': 982, 'runs': 983, 'quiet': 984, 'plus': 985, 'major': 986, 'design': 987, 'flaw': 988, 'heavy': 989, 'bulky': 990, 'included': 991, 'last': 992, 'years': 993, 'thats': 994, 'spanned': 995, 'electronic': 996, 'Three': 997, 'advantages': 998, 'pcs': 999, \"'\": 1000, 'linux': 1001, 'based': 1002, 'os': 1003, 'simply': 1004, 'makes': 1005, 'sense': 1006, 'easier': 1007, 'extra': 1008, 'expense': 1009, 'necessary': 1010, 'screams': 1011, 'unique': 1012, 'OSX': 1013, '16': 1014, 'functions': 1015, 'routed': 1016, 'rather': 1017, 'something': 1018, 'Core': 1019, 'processors': 1020, 'HDMI': 1021, 'port': 1022, 'hook': 1023, 'directly': 1024, 'TV': 1025, 'looking': 1026, 'cash': 1027, 'enough': 1028, 'aftermarket': 1029, 'LaCie': 1030, '2Big': 1031, 'firewire': 1032, '800': 1033, 'interface': 1034, 'useful': 1035, 'Time': 1036, 'Machine': 1037, 'pleased': 1038, 'toshiba': 1039, 'satellite': 1040, 'home': 1041, 'shorter': 1042, 'notch': 1043, 'image': 1044, 'soud': 1045, 'excelent': 1046, 'Very': 1047, 'investment': 1048, 'Upgrading': 1049, 'Starter': 1050, 'Home': 1051, 'Premium': 1052, 'Professional': 1053, 'Lion': 1054, 'kind': 1055, 'getting': 1056, 'glary': 1057, 'clicking': 1058, 'buttons': 1059, 'them': 1060, 'Not': 1061, 'Now': 1062, 'care': 1063, 'unsteady': 1064, 'feet': 1065, 'susceptible': 1066, 'bugs': 1067, 'DO': 1068, 'GATEWAY': 1069, 'THEY': 1070, 'ARE': 1071, 'JUNK': 1072, 'AND': 1073, 'THE': 1074, 'COMPANY': 1075, 'HORRIBLE': 1076, '/': 1077, 'weight': 1078, 'aluminum': 1079, 'apps': 1080, 'Facebook': 1081, 'watching': 1082, 'movies': 1083, 'From': 1084, 'build': 1085, 'sub-par': 1086, 'dock': 1087, 'where': 1088, 'file': 1089, 'ontop': 1090, 'particular': 1091, 'pretty': 1092, 'else': 1093, 'stops': 1094, 'truly': 1095, 'costing': 1096, 'thousand': 1097, 'bucks': 1098, 'before': 1099, 'Waiting': 1100, 'i7': 1101, 'value': 1102, 'Boots': 1103, 'Lightweight': 1104, 'beautiful': 1105, 'Macbook': 1106, 'arrived': 1107, 'twin': 1108, 'packing': 1109, 'sealed': 1110, 'sorry': 1111, 'die': 1112, 'led': 1113, 'Best': 1114, 'Buy': 1115, 'promise': 1116, 'tech': 1117, 'received': 1118, 'beyond': 1119, 'call': 1120, 'duty': 1121, 'priced': 1122, 'crash': 1123, 'burn': 1124, 'etc': 1125, 'probably': 1126, 'Reason': 1127, 'why': 1128, 'lose': 1129, 'decides': 1130, 'mode': 1131, '10-20': 1132, '%': 1133, 'unlike': 1134, '80': 1135, '17': 1136, 'inch': 1137, 'dropping': 1138, 'stock': 1139, 'market': 1140, 'given': 1141, 'horrible': 1142, 'another': 1143, 'plunge': 1144, 'For': 1145, 'Bluetooth': 1146, 'must': 1147, 'Launch': 1148, 'Manager': 1149, 'Drivers/Applications': 1150, 'show': 1151, 'reload': 1152, 'switchable': 1153, 'graphic': 1154, 'sweet': 1155, 'decreased': 1156, 'thrilled': 1157, 'tried': 1158, '35': 1159, 'offer': 1160, 'technical': 1161, 'overload': 1162, 'updates': 1163, 'BOOT': 1164, 'MGR': 1165, 'come': 1166, '25': 1167, 'cds': 1168, 'met': 1169, 'Skype': 1170, 'dang': 1171, 'HDD': 1172, 'secures': 1173, 'inside': 1174, 'rails': 1175, 'main': 1176, 'All': 1177, 'grand': 1178, 'hunk': 1179, 'crap': 1180, 'Vaio': 1181, 'Games': 1182, 'invalidate': 1183, 'understand': 1184, 'anyway': 1185, 'business': 1186, 'consider': 1187, 'MS': 1188, 'Office': 1189, 'trial': 1190, 'versions': 1191, 'hope': 1192, 'own': 1193, 'copies': 1194, 'Even': 1195, 'dualboot': 1196, 'amazingly': 1197, '7hrs': 1198, '5hrs': 1199, 'transition': 1200, 'Nortons': 1201, 'scan': 1202, 'others': 1203, 'Buyer': 1204, 'amazed': 1205, 'Came': 1206, 'fully': 1207, 'noticed': 1208, 'investing': 1209, 'broad': 1210, 'letters': 1211, 'lighter': 1212, 'complete': 1213, 'opposite': 1214, 'ergonomic': 1215, 'sometimes': 1216, 'Wireless': 1217, 'meantioned': 1218, 'Notebooks': 1219, 'background': 1220, 'knowlede': 1221, 'navigating': 1222, 'quicker': 1223, 'speakers': 1224, 'subwoofer': 1225, 'Theres': 1226, 'built': 1227, 'camera': 1228, 'special': 1229, 'effects-': 1230, 'photography': 1231, 'left': 1232, 'broke': 1233, 'costed': 1234, 'plenty': 1235, 'space': 1236, 'long-life': 1237, '10-11': 1238, 'depending': 1239, 'configure': 1240, 'level': 1241, 'settings': 1242, 'black': 1243, 'model': 1244, 'seamless': 1245, 'appearance': 1246, 'notebooks': 1247, 'seen': 1248, 'dell': 1249, '3rd': 1250, 'world': 1251, 'bother': 1252, 'awful': 1253, 'cheap': 1254, 'payment': 1255, 'malfunction': 1256, 'comfortable': 1257, 'yourself': 1258, 'case': 1259, 'happy': 1260, 'looks': 1261, 'Excellent': 1262, 'data': 1263, 'into': 1264, 'finances': 1265, 'scheduling': 1266, 'parents': 1267, 'expenses': 1268, 'definitely': 1269, 'taught': 1270, 'Photoshop': 1271, 'driver': 1272, 'doesnt': 1273, 'damn': 1274, 'Bigger': 1275, 'bid': 1276, 'chatting': 1277, 'techs': 1278, 'remote': 1279, 'buildings': 1280, 'campus': 1281, 'burning': 1282, 'during': 1283, 'fine': 1284, 'wireless': 1285, 'Product': 1286, 'each': 1287, 'costs': 1288, 'distan': 1289, 'Overall': 1290, 'loves': 1291, 'casing': 1292, 'fried': 1293, 'shocked': 1294, 'husband': 1295, 'he': 1296, 'pulled': 1297, 'socket': 1298, 'signals': 1299, 'matter': 1300, 'bring': 1301, 'pro': 1302, 'different': 1303, 'may': 1304, 'huge': 1305, 'tag': 1306, 'actually': 1307, 'future': 1308, 'multiple': 1309, 'page': 1310, 'viewer': 1311, 'press': 1312, 'button': 1313, 'separate': 1314, 'currently': 1315, 'opened': 1316, 'non': 1317, 'stop': 1318, 'shopping': 1319, 'Keyboard': 1320, 'sized': 1321, 'Im': 1322, 'WHEN': 1323, 'TYPING': 1324, 'LETTERS': 1325, 'SPACES': 1326, 'FREQUENTLY': 1327, 'OMITTED': 1328, 'REQUIRING': 1329, 'USER': 1330, 'REDO': 1331, 'MANY': 1332, 'WORDS': 1333, 'SENTENCES': 1334, 'Only': 1335, 'days': 1336, 'froze': 1337, 'ideal': 1338, 'children': 1339, 'temp': 1340, 'fault': 1341, 'fabulous': 1342, 'improvements': 1343, 'existing': 1344, 'line': 1345, 'bumping': 1346, 'adding': 1347, 'thunderbolt': 1348, 'Images': 1349, 'clean': 1350, 'although': 1351, 'vista': 1352, 'compared': 1353, 'xp': 1354, 'sucks': 1355, 'iPhoto': 1356, 'iWeb': 1357, 'helpful': 1358, 'At': 1359, 'worked': 1360, 'failed': 1361, 'three': 1362, 'weeks': 1363, 'later': 1364, 'receive': 1365, 'discover': 1366, 'Until': 1367, 'looked': 1368, 'options': 1369, 'deal': 1370, 'girlfriend': 1371, 'hinge': 1372, 'loose': 1373, 'close': 1374, 'LCD': 1375, 'Customer': 1376, 'Service': 1377, 'Newegg': 1378, 'RMA': 1379, 'contacted': 1380, 'late': 1381, 'Friday': 1382, 'issued': 1383, 'number': 1384, 'PrePaid': 1385, 'UPS': 1386, 'label': 1387, 'morning': 1388, 'Saturday': 1389, 'Design': 1390, 'durable': 1391, 'ONLY': 1392, 'screen/video': 1393, '1024': 1394, 'x': 1395, '60': 1396, 'hiccups': 1397, 'uploading': 1398, 'photos': 1399, '720p': 1400, 'occasion': 1401, 'creating': 1402, 'presentations': 1403, 'Display': 1404, 'computer/printer': 1405, 'found': 1406, 'TAB': 1407, 'functioning': 1408, '-Called': 1409, 'headquarters': 1410, 'TFT': 1411, 'panel': 1412, 'broken': 1413, 'week': 1414, 'Navigation': 1415, 'superior': 1416, 'framed': 1417, 'half-': 1418, 'full-inch': 1419, 'margin': 1420, 'obviously': 1421, 'reduces': 1422, 'increases': 1423, 'bulk': 1424, 're-install': 1425, 'cracks': 1426, 'hinges': 1427, 'SECOND': 1428, 'PROBLEM': 1429, 'INVOLVES': 1430, 'BATTERY': 1431, 'WHICH': 1432, 'ADVERTISED': 1433, 'AS': 1434, 'HAVING': 1435, 'STORAGE': 1436, 'LIFE': 1437, '11': 1438, 'HOURS': 1439, 'BUT': 1440, 'FULLY': 1441, 'CHARGED': 1442, 'SHOWS': 1443, '250': 1444, 'gb': 1445, 'upgrade': 1446, 'ram..': 1447, 'Programs': 1448, 'unstable': 1449, 'bindings': 1450, 'loved': 1451, 'reasonable': 1452, 'noise': 1453, 'constant': 1454, 'hissing': 1455, 'solid': 1456, 'novice': 1457, '.1': 1458, 'ghz': 1459, '500gb': 1460, 'big': 1461, 'DVDs': 1462, 'media': 1463, 'USB': 1464, 'output': 1465, 'absolute': 1466, 'niece': 1467, 'nephew': 1468, 'played': 1469, 'require': 1470, 'dedicated': 1471, '2': 1472, 'instead': 1473, 'Typically': 1474, 'convenience': 1475, 'super': 1476, 'Is': 1477, 'access': 1478, 'treated': 1479, 'company': 1480, 'Small': 1481, 'flight': 1482, 'Light': 1483, 'airports': 1484, 'powerful': 1485, 'trips': 1486, 'Typing': 1487, 'uncomfortable': 1488, 'edges': 1489, 'wrists': 1490, 'rest': 1491, 'ILife': 1492, 'iPhotos': 1493, 'anybody': 1494, 'wants': 1495, 'relax': 1496, 'enraged': 1497, 'cursing': 1498, 'gods': 1499, 'throw': 1500, 'door': 1501, 'exactly': 1502, '2.4': 1503, 'GHz': 1504, 'C2D': 1505, 'antiquated': 1506, 'CPU': 1507, 'thus': 1508, 'occasional': 1509, 'spinning': 1510, 'wheel': 1511, 'appear': 1512, 'Word': 1513, 'Excel': 1514, 'bummer': 1515, 'possible': 1516, 'geek': 1517, 'originally': 1518, 'concerned': 1519, 'view': 1520, 'college': 1521, 'formatting': 1522, 'learn': 1523, 'allow': 1524, 'convert': 1525, 'documents': 1526, 'readable': 1527, 'Macs': 1528, 'Runs': 1529, 'regular': 1530, 'layout': 1531, 'straight': 1532, 'organized': 1533, 'strive': 1534, 'myself': 1535, 'school': 1536, 'Support': 1537, 'lackluster': 1538, 'refund': 1539, '>': 1540, 'convenient': 1541, 'write': 1542, 'html': 1543, 'programming': 1544, 'both': 1545, 'Needs': 1546, 'More': 1547, 'Hrs': 1548, 'May': 1549, 'actual': 1550, 'Net': 1551, 'operates': 1552, 'real': 1553, 'pictures': 1554, 'transporting': 1555, 'flaws': 1556, 'gamers': 1557, 'worlds': 1558, 'worst': 1559, 'repair': 1560, 'AfterEffects': 1561, 'pc': 1562, 'examined': 1563, 'cited': 1564, 'anyways': 1565, 'already': 1566, 'dependability': 1567, 'shown': 1568, 'worse': 1569, 'nightmare': 1570, 'onto': 1571, 'owned': 1572, 'labtop': 1573, 'overheating': 1574, 'occational': 1575, 'blue': 1576, 'ridiculously': 1577, 'thinking': 1578, 'half': 1579, 'Good': 1580, 'largest': 1581, 'effective': 1582, 'webpages': 1583, 'eighty': 1584, 'lifelong': 1585, 'wiped': 1586, 'OpenOffice': 1587, 'Firefox': 1588, 'efficient': 1589, 'inexpensive': 1590, '400': 1591, 'MY': 1592, 'CAN': 1593, 'REG': 1594, 'PRODUCT': 1595, 'KEY': 1596, 'Sure': 1597, 'seemingly': 1598, 'high-end': 1599, 'specs': 1600, 'Love': 1601, 'stability': 1602, 'capable': 1603, 'moderate': 1604, 'boost': 1605, 'larger': 1606, 'Pros': 1607, 'mobile': 1608, 'ram': 1609, 'Would': 1610, 'trendy': 1611, 'primary': 1612, 'secondary': 1613, 'control': 1614, 'graphically': 1615, 'functionally': 1616, 'gives': 1617, 'projects': 1618, 'seamlessly': 1619, 'sowish': 1620, 'Awesome': 1621, 'statement': 1622, 'mere': 1623, 'exaggeration': 1624, 'lie': 1625, 'relatively': 1626, 'informative': 1627, 'wonder': 1628, 'alerts': 1629, 'tutorials': 1630, 'someone': 1631, 'technologically': 1632, 'advanced': 1633, 'various': 1634, 'stupid': 1635, 'pop': 1636, 'ups': 1637, 'blocked': 1638, 'wait': 1639, 'webpage': 1640, 'slowing': 1641, 'down': 1642, 'Memory': 1643, 'upgradable': 1644, 'Web': 1645, '3G': 1646, 'network': 1647, 'VERY': 1648, 'DISAPPOINTING': 1649, 'Adobe': 1650, 'Creative': 1651, 'Suite': 1652, 'heard': 1653, 'lacking': 1654, 'improving': 1655, 'plug': 1656, 'Second': 1657, 'cover': 1658, 'walls': 1659, 'iphoto': 1660, 'amateurs': 1661, 'experts': 1662, 'alike': 1663, 'STARTER': 1664, 'Ever': 1665, 'nothing': 1666, 'downs': 1667, 'services': 1668, 'hotlines': 1669, 'noisy': 1670, 'click': 1671, 'storing': 1672, 'organizing': 1673, 'Other': 1674, 'performing': 1675, 'capabilities': 1676, 'imail': 1677, 'incorporate': 1678, 'address': 1679, 'ipad': 1680, 'imovie': 1681, 'Comfortable': 1682, 'transport': 1683, 'Computer': 1684, 'depot': 1685, 'stinks': 1686, 'fall': 1687, 'friend': 1688, 'disk': 1689, 'capacity': 1690, 'scroll': 1691, 'window': 1692, 'fingers': 1693, 'point': 1694, 'personally': 1695, 'delivered': 1696, 'professional': 1697, 'front': 1698, 'seem': 1699, 'downfall': 1700, 'shut': 1701, 'decided': 1702, 'connectivity': 1703, 'important': 1704, 'net': 1705, '50': 1706, 'read': 1707, 'discharges': 1708, 'honest': 1709, 'compatibility': 1710, 'quirks': 1711, 'll': 1712, 'ceased': 1713, 'heat': 1714, 'coming': 1715, 'however': 1716, 'suggest': 1717, 'purchasing': 1718, 'nature': 1719, 'sink': 1720, 'Webcam': 1721, 'laggy': 1722, 'greatest': 1723, 'except': 1724, 'plastic': 1725, 'piece': 1726, 'covers': 1727, 'wires': 1728, 'Wonderful': 1729, 'outside': 1730, 'speedy': 1731, 'wirelessly': 1732, 'regardless': 1733, 'connection': 1734, 'weak': 1735, 'pricing': 1736, 'competitive': 1737, 'exceptionally': 1738, 'thin': 1739, 'Maybe': 1740, 'related': 1741, 'locked': 1742, 'occasions': 1743, 'gone': 1744, 'vivid': 1745, 'typers': 1746, 'IWORKS': 1747, 'Itunes': 1748, 'Email': 1749, 'printers': 1750, 'perfectly': 1751, 'macbooks': 1752, 'tons': 1753, 'information': 1754, 'mousepad': 1755, 'pain': 1756, 'arse': 1757, 'minor': 1758, 'Registration/1st': 1759, 'Take': 1760, 'patiently': 1761, 'trimming': 1762, 'Returned': 1763, '2nd': 1764, 'obvious': 1765, 'physical': 1766, 'damage': 1767, 'bulging': 1768, 'speaker': 1769, 'grill': 1770, 'pressed': 1771, 'inoperative': 1772, 'letter': 1773, 'Most': 1774, 'tutoring': 1775, 'bouncing': 1776, 'student': 1777, 'portability': 1778, 'advertised': 1779, 'condition': 1780, 'frozen': 1781, 'screens': 1782, 'kept': 1783, 'happening': 1784, 'downloads': 1785, 'completed': 1786, 'said': 1787, \"'corrupted\": 1788, 'Horrible': 1789, 'aesthetics': 1790, 'downside': 1791, 'stare': 1792, 'five': 1793, 'headphone': 1794, 'jack': 1795, 'touch-pad': 1796, 'headphones/mic': 1797, 'handed': 1798, 'person': 1799, '[': 1800, ']': 1801, 'What': 1802, 'additional': 1803, 'booting': 1804, 'shutting': 1805, 'serious': 1806, 'complain': 1807, 'darker': 1808, 'everywhere': 1809, 'stuff': 1810, 'verion': 1811, 'asks': 1812, 'cancel': 1813, 'wont': 1814, 'let': 1815, 'processer': 1816, 'solve': 1817, 'closely': 1818, 'comparing': 1819, 'granted': 1820, 'standard': 1821, 'return': 1822, 'touch-mouse': 1823, 'responding': 1824, 'mark': 1825, 'arm': 1826, 'velcro': 1827, 'torn': 1828, 'here': 1829, 'hence': 1830, 'irritating': 1831, 'slide': 1832, 'downwards': 1833, 'GB': 1834, 'RAM': 1835, 'slowdown': 1836, 'versitility': 1837, 'outstanding': 1838, 'beautifully': 1839, 'loving': 1840, 'Netbook': 1841, 'openning': 1842, 'pages': 1843, 'installing': 1844, 'browsers': 1845, 'eventually': 1846, 'giving': 1847, 'everytime': 1848, 'booted': 1849, 'Slow': 1850, 'Charger': 1851, 'seems': 1852, 'class': 1853, 'dislike': 1854, 'placement': 1855, 'clarity': 1856, 'sharpness': 1857, 'safety': 1858, 'affect': 1859, 'hide': 1860, 'message': 1861, 'unibody': 1862, 'edgy': 1863, 'STOPPED': 1864, 'BOOTING': 1865, 'UP': 1866, 'one-year': 1867, 'winner': 1868, 'HSN': 1869, 'turns': 1870, 'often': 1871, '...': 1872, '..': 1873, 'Strong': 1874, 'device': 1875, 'fun': 1876, 'strong': 1877, 'era': 1878, 'serves': 1879, 'modern': 1880, 'requirements': 1881, 'game': 1882, 'designers': 1883, 'sorely': 1884, 'disapointed': 1885, 'reputable': 1886, 'honor': 1887, 'blade': 1888, 'fell': 1889, 'apart': 1890, 'functionality': 1891, 'simplicity': 1892, 'HOT': 1893, 'scary': 1894, 'despite': 1895, 'turning': 1896, 'lower': 1897, 'mute': 1898, 'reproduction': 1899, 'expect': 1900, 'He': 1901, 'schools': 1902, 'teaching': 1903, 'kids': 1904, 'him': 1905, 'Getting': 1906, 'Care': 1907, 'plan': 1908, 'recomend': 1909, 'tell': 1910, 'dv4': 1911, 'boasted': 1912, 'bigger': 1913, 'nicer': 1914, '1.5-2.0': 1915, 'hrs': 1916, 'stellar': 1917, 'Aside': 1918, 'lack': 1919, 'previous': 1920, 'gray': 1921, 'parts': 1922, 'reliability': 1923, 'recenlty': 1924, 'purchased': 1925, 'compatible': 1926, 'save': 1927, 'pre': 1928, 'returned': 1929, 'unrepaired': 1930, '176': 1931, 'shot': 1932, 'grafics': 1933, 'ATI': 1934, '5870': 1935, '8GB': 1936, 'LED': 1937, 'biggest': 1938, 'instructions': 1939, 'similarly': 1940, 'Screen': 1941, 'operate': 1942, 'gadgets': 1943, 'Rolls': 1944, 'Royce': 1945, 'deep': 1946, 'unable': 1947, 'create': 1948, 'track': 1949, 'stands': 1950, 'Geek': 1951, 'Squad': 1952, 'accidently': 1953, 'correct': 1954, 'First': 1955, 'known': 1956, 'rarely': 1957, 'translates': 1958, 's': 1959, 'Battery': 1960, 'apple': 1961, 'reputation': 1962, 'usage': 1963, 'unit': 1964, 'locking': 1965, 'Garageband': 1966, 'default': 1967, 'cant': 1968, 'spending': 1969, 'jump': 1970, 'island': 1971, 'alone': 1972, 'experienced': 1973, 'realizwed': 1974, 'communicate': 1975, 'family': 1976, 'members': 1977, 'secure': 1978, 'weary': 1979, 'almost': 1980, 't': 1981, 'gotten': 1982, 'current': 1983, 'cards': 1984, 'choice': 1985, 'Apparently': 1986, 'manufacturing': 1987, 'defect': 1988, 'thermal': 1989, 'paste': 1990, 'Asus': 1991, 'facial': 1992, 'recognition': 1993, 'logon': 1994, 'either': 1995, 'unusable': 1996, 'ask': 1997, 'give': 1998, 'scale': 1999, 'basics': 2000, 'crutchs': 2001, 'wheelchair': 2002, 'backpack': 2003, 'bag': 2004, 'Can': 2005, 'watch': 2006, 'videos': 2007, 'hardly': 2008, 'preformed': 2009, 'minute': 2010, 'fire': 2011, 'contact': 2012, 'supplied': 2013, 'serial': 2014, 'delay': 2015, 'sending': 2016, 'expired': 2017, 'replacing': 2018, 'persoal': 2019, 'models': 2020, 'hurt': 2021, 'sucked': 2022, 'juice': 2023, 'SOL': 2024, 'covering': 2025, \"I'ts\": 2026, 'higher-end': 2027, 'fits': 2028, 'budget': 2029, 'Was': 2030, 'disappointed': 2031, 'discontinued': 2032, 'apparently': 2033, 'LOVE': 2034, 'THIS': 2035, 'LAPTOP': 2036, 'WONDERFUL': 2037, 'PRICE': 2038, 'FOR': 2039, 'WHAT': 2040, 'GET': 2041, 'contributor': 2042, 'art': 2043, 'aspect': 2044, 'Accordingly': 2045, 'NEVER': 2046, 'Compaq': 2047, 'lasted': 2048, '5-years': 2049, 'everyone': 2050, 'thinks': 2051, 'overpriced': 2052, 'overrated': 2053, 'initial': 2054, 'penny': 2055, 'besides': 2056, 'financing': 2057, 'interested': 2058, 'selling': 2059, 'warranties': 2060, 'helping': 2061, 'fixing': 2062, 'junk': 2063, 'System': 2064, 'loosing': 2065, '20': 2066, 'difference': 2067, 'tim': 2068, 'dream': 2069, 'overheated': 2070, 'slightly': 2071, 'warm': 2072, 'ton': 2073, 'compliments': 2074, 'speaking': 2075, 'regularly': 2076, 'commutes': 2077, 'bus': 2078, 'attest': 2079, 'restricted': 2080, 'width': 2081, 'computing': 2082, 'room': 2083, 'taken': 2084, 'anywhere': 2085, 'headsets': 2086, 'instance': 2087, 'okay': 2088, 'expeirencing': 2089, 'failing': 2090, 'powering': 2091, 'freezing': 2092, 'sorts': 2093, 'narcissist': 2094, 'staff': 2095, 'resetting': 2096, 'sticks': 2097, 'tools': 2098, 'fit': 2099, 'resturant': 2100, 'food': 2101, 'table': 2102, 'consuming': 2103, 'True': 2104, 'supposed': 2105, 'LG': 2106, 'center': 2107, 'provide': 2108, 'Material': 2109, 'Specs': 2110, 'performs': 2111, 'extensive': 2112, 'research': 2113, 'macconnection': 2114, 'lowest': 2115, '15': 2116, 'MBP': 2117, 'i5': 2118, 'ADDICTED': 2119, 'photo': 2120, 'booth': 2121, 'Hard': 2122, 'editions': 2123, '500GB': 2124, '320GB': 2125, 'trust': 2126, 'internal': 2127, 'connects': 2128, 'WIFI': 2129, 'library': 2130, 'elsewhere': 2131, 'considered': 2132, 'mainboard': 2133, 'NOTHING': 2134, 'manually': 2135, 'accessories': 2136, 'satisfactory': 2137, 'aggrevation': 2138, 'To': 2139, 'faulty': 2140, 'equipment': 2141, 'idk': 2142, 'tool': 2143, 'academic': 2144, 'studies': 2145, 'wt': 2146, 'solidly': 2147, 'Sound': 2148, 'limited': 2149, '-Computer': 2150, 'frequently': 2151, '-I': 2152, 'propose': 2153, 'swap': 2154, 'drives': 2155, 'Operating': 2156, 'AC': 2157, 'Peformance': 2158, 'Summary': 2159, 'BIOS': 2160, 'proactive': 2161, 'resolve': 2162, 'gradual': 2163, 'substantive': 2164, 'Many': 2165, 'classmates': 2166, 'Ease': 2167, 'benefits': 2168, 'CA': 2169, 'sheer': 2170, 'flexibility': 2171, 'techie': 2172, 'S-video': 2173, 'enable': 2174, '6': 2175, 'surfing': 2176, 'Sundays': 2177, 'checking': 2178, 'football': 2179, 'scores': 2180, 'funny': 2181, 'Youtube': 2182, 'Iphoto': 2183, 'facebook': 2184, 'social': 2185, 'networking': 2186, 'sites': 2187, 'Some': 2188, 'arent': 2189, 'equal': 2190, 'began': 2191, 'diminish': 2192, 'company-wide': 2193, 'recall': 2194, 'overheats': 2195, 'artist': 2196, 'overnight': 2197, 'apologized': 2198, 'networks': 2199, 'rep': 2200, 'answer': 2201, 'question': 2202, 'understood': 2203, 'spoke': 2204, 'english': 2205, 'try': 2206, 'acknowledge': 2207, 'Again': 2208, 'starting': 2209, 'reinstall': 2210, 'proprietary': 2211, 'forgot': 2212, 'required': 2213, 'import': 2214, 'paperwork': 2215, 'mom': 2216, 'she': 2217, 'reports': 2218, 'lasts': 2219, 'her': 2220, 'response': 2221, 'Internet': 2222, 'focused': 2223, 'activity': 2224, 'mail': 2225, 'sophisticated': 2226, 'charges': 2227, 'medium': 2228, 'attracted': 2229, 'specifications': 2230, 'THEN': 2231, 'ready': 2232, 'tired': 2233, 'printer': 2234, 'attractive': 2235, 'tightened': 2236, 'produces': 2237, 'finished': 2238, 'shop': 2239, 'push': 2240, 'mostly': 2241, 'confident': 2242, 'live': 2243, 'appreciate': 2244, 'price-point': 2245, 'boots': 2246, 'luxurys': 2247, 'ar': 2248, 'impressive': 2249, 'name': 2250, 'pros': 2251, 'Disappointing': 2252, 'lovely': 2253, 'batteries': 2254, 'Student': 2255, 'Edition': 2256, 'six': 2257, 'sticking': 2258, 'correctly': 2259, 'equipped': 2260, 'Without': 2261, 'doubt': 2262, '*': 2263, 'fantastic': 2264, 'ten': 2265, 'essay': 2266, 'papers': 2267, 'semi-decent': 2268, '24/7': 2269, 'iBookG4': 2270, 'please': 2271, 'visually': 2272, 'appealing': 2273, 'FIRST': 2274, 'THAT': 2275, 'KEYBOARD': 2276, 'FUNCTION': 2277, 'SIMPLY': 2278, 'UNSATISFACTORY': 2279, 'musicians': 2280, 'microphone': 2281, 'beginners': 2282, 'intermediate': 2283, 'Linux': 2284, 'security-prone': 2285, 'difficulty': 2286, 'migration': 2287, 'cable': 2288, 'iBook': 2289, 'Seems': 2290, 'shipment': 2291, 'innovations': 2292, 'quicklook': 2293, 'heaps': 2294, 'blown': 2295, 'keeper': 2296, 'stylish': 2297, 'array': 2298, 'choose': 2299, 'goes': 2300, 'PhotoBooth': 2301, 'kinds': 2302, 'downloaded': 2303, 'bluescreened': 2304, 'warning': 2305, 'basic': 2306, 'Chrome': 2307, 'researched': 2308, 'MacConnection': 2309, 'dvd': 2310, 'COMPUTER': 2311, 'PORTABILITY': 2312, 'FAST': 2313, 'PROCESSING': 2314, 'Supplied': 2315, 'greatly': 2316, 'welcomed': 2317, 'inept': 2318, 'No': 2319, 'tey': 2320, 'bios': 2321, 'How': 2322, 'decent': 2323, 'insanity': 2324, 'shipped': 2325, 'Info': 2326, 'kernal': 2327, 'corrupt': 2328, 'general': 2329, 'In': 2330, 'moth': 2331, 'hardrive': 2332, 'WIth': 2333, 'Simple': 2334, 'spectacular': 2335, 'claim': 2336, 'uses': 2337, 'screws': 2338, 'moved': 2339, 'forth': 2340, 'Downfalls': 2341, 'enjoyed': 2342, 'suspicious': 2343, 'Unable': 2344, 'addition': 2345, 'Pages': 2346, 'Keynotes': 2347, 'twice': 2348, 'FINALLY': 2349, 'agreed': 2350, 'ALL': 2351, 'IN': 2352, 'LESS': 2353, 'THAN': 2354, 'YEARS': 2355, 'says': 2356, 'occurance': 2357, 'Browsing': 2358, 'itunes': 2359, 'slows': 2360, 'comp': 2361, 'indirectly': 2362, 'tranferring': 2363, 'country': 2364, 'kidding': 2365, 'story': 2366, 'history': 2367, 'list': 2368, 'calls': 2369, 'surprised': 2370, 'picks': 2371, 'High': 2372, 'definition': 2373, 'Beast': 2374, 'wall': 2375, 'cause': 2376, 'environment': 2377, 'desktops': 2378, 'component': 2379, 'recently': 2380, 'Pentium': 2381, 'HT': 2382, 'SATA': 2383, 'rock': 2384, 'star': 2385, 'printing': 2386, 'adventure': 2387, 'laughable': 2388, 'chess': 2389, 'expese': 2390, 'realize': 2391, 'bookkeeping': 2392, 'aero': 2393, 'get-go': 2394, 'M6809': 2395, 'daily': 2396, 'compact': 2397, 'Half-Life': 2398, 'World': 2399, 'Warcraft': 2400, 'handle': 2401, 'preferred': 2402, 'smaller': 2403, 'regret': 2404, 'understanding': 2405, 'Dealing': 2406, 'drone': 2407, 'torture': 2408, 'Final': 2409, 'Cut': 2410, 'seemlessly': 2411, 'transfer': 2412, 'whether': 2413, 'images': 2414, 'feels': 2415, 'sitting': 2416, 'lap': 2417, 'desk': 2418, 'Lenovo': 2419, 'tablet': 2420, 'unplug': 2421, 'objection': 2422, 'starter': 2423, 'happier': 2424, 'transcription': 2425, 'flatline': 2426, 'typing': 2427, 'Speakers': 2428, 'abuse': 2429, 'pushed': 2430, 'Explorer': 2431, 'beginning': 2432, 'admit': 2433, 'macBook': 2434, 'job': 2435, 'gold': 2436, 'numbers': 2437, 'intall': 2438, 'accept': 2439, 'weighed': 2440, 'Enjoy': 2441, 'Toshib': 2442, 'durability': 2443, 'unparalleled': 2444, 'Which': 2445, 'Business': 2446, 'scored': 2447, '5.X': 2448, 'index': 2449, 'total': 2450, 'score': 2451, 'tote': 2452, 'irreplaceable': 2453, 'downloading': 2454, 'worried': 2455, 'protected': 2456, 'Norton': 2457, 'locekd': 2458, 'command': 2459, 'prompt': 2460, 'Could': 2461, '2GB': 2462, 'held': 2463, 'holding': 2464, 'neat': 2465, 'icon': 2466, 'welcome': 2467, 'cluttered': 2468, 'confusing': 2469, 'icons': 2470, 'winning': 2471, 'combination': 2472, 'sensitive': 2473, 'dead': 2474, 'pixels': 2475, 'upper': 2476, 'zone': 2477, 'noticable': 2478, 'astonishing': 2479, 'picture': 2480, 'i3': 2481, 'See': 2482, 'normal': 2483, 'trials': 2484, 'Yahoo': 2485, 'Mail': 2486, 'travel': 2487, 'brightness': 2488, 'automatically': 2489, 'adjusts': 2490, 'Besides': 2491, 'plane': 2492, 'overcome': 2493, 'pair': 2494, 'head': 2495, 'Right': 2496, 'reflectiveness': 2497, 'inconvenience': 2498, 'controlled-lighting': 2499, 'prefer': 2500, 'crank': 2501, 'complaint': 2502, 'fuzz': 2503, 'headphones': 2504, 'rated': 2505, 'ok': 2506, 'Treat': 2507, 'long-lasting': 2508, 'ordering': 2509, 'sporting': 2510, 'gigs': 2511, 'surf': 2512, 'edit': 2513, 'non-functioning': 2514, 'Fast': 2515, 'visual': 2516, 'alright': 2517, 'plate': 2518, 'hollow': 2519, 'odd': 2520, 'reasons': 2521, 'recognize': 2522, 'Applications': 2523, 'seconds': 2524, 'lags': 2525, 'awkward': 2526, 'moments': 2527, 'tea': 2528, 'respond': 2529, 'February': 2530, 'Warrenty': 2531, 'fighting': 2532, 'agents': 2533, 'NONE': 2534, 'English': 2535, 'satisfy': 2536, 'acually': 2537, 'Nvidia': 2538, 'requires': 2539, 'lighted': 2540, 'dump': 2541, 'belongs': 2542, 'trash': 2543, 'finding': 2544, 'exist': 2545, 'repairs': 2546, 'incorrect': 2547, 'pointer': 2548, 'EITHER': 2549, 'WAY': 2550, 'Well': 2551, 'pocket': 2552, 'engineering': 2553, 'maneuver': 2554, 'option': 2555, 'comfortably': 2556, 'dorm': 2557, 'household': 2558, 'inspected': 2559, 'netbooks': 2560, 'clearly': 2561, 'tighter': 2562, 'demonstrate': 2563, 'Did': 2564, 'Walmart.com': 2565, 'exchanged': 2566, 'make/model': 2567, 'Keynote': 2568, 'Numbers': 2569, 'eliminates': 2570, 'greater': 2571, 'dissuade': 2572, 'planet': 2573, 'wasted': 2574, 'patches': 2575, 'de-corrupt': 2576, 'infested': 2577, 'anyhow': 2578, 'u': 2579, 'compares': 2580, '2000': 2581, 'google': 2582, 'coupons': 2583, 'codes': 2584, 'significant': 2585, 'defeats': 2586, 'purpose': 2587, 'keeping': 2588, 'Unfortunately': 2589, 'moving': 2590, 'finger': 2591, 'move': 2592, 'rate': 2593, 'considering': 2594, 'span': 2595, 'implore': 2596, 'wonderfull': 2597, 'students': 2598, 'document': 2599, 'creation': 2600, 'iwork': 2601, 'blame': 2602, 'stick': 2603, 'recomended': 2604, 'Externally': 2605, 'falling': 2606, 'rubbing': 2607, 'below': 2608, 'heals': 2609, 'hands': 2610, 'sit': 2611, 'glare': 2612, 'lightscribe': 2613, 'application': 2614, 'ince': 2615, 'bags': 2616, 'multitude': 2617, 'update': 2618, 'disappointing': 2619, 'learned': 2620, 'knapsack': 2621, 'text': 2622, 'books': 2623, 'adds': 2624, 'noises': 2625, 'bottom': 2626, 'items': 2627, 'technician': 2628, 'house': 2629, 'defective': 2630, 'challenging': 2631, 'Cords': 2632, 'cords': 2633, 'messy': 2634, 'setup': 2635, 'direction': 2636, 'Unless': 2637, 'inconvenienced': 2638, 'attachment': 2639, 'Core2': 2640, 'Quad': 2641, '2.83': 2642, 'mobilize': 2643, 'angles': 2644, 'reacts': 2645, 'lighting': 2646, 'luxury': 2647, '-when': 2648, 'areas': 2649, 'privacy': 2650, 'softer': 2651, 'lit': 2652, 'Looks': 2653, 'horribly': 2654, 'willing': 2655, 'base': 2656, 'Win': 2657, 'mentioned': 2658, 'incase': 2659, 'shells': 2660, 'accommodates': 2661, 'Called': 2662, 'ship': 2663, 'Had': 2664, 'lost': 2665, 'streamlined': 2666, 'randomely': 2667, 'refuse': 2668, 'safe': 2669, 'over-sized': 2670, '18-inch': 2671, '100': 2672, 'universal': 2673, '300': 2674, 'powerpoint': 2675, 'projector': 2676, '1.You': 2677, 'function': 2678, 'Strengths': 2679, 'shaped': 2680, 'Weaknesses': 2681, 'videocard': 2682, 'begin': 2683, 'REAL': 2684, 'toy': 2685, 'saw': 2686, 'brands': 2687, 'Price': 2688, 'zooming': 2689, 'colorful': 2690, 'backlighting': 2691, 'honesty': 2692, 'Worse': 2693, '*netbook*': 2694, 'outperforms': 2695, 'luck': 2696, 'phone-Visited': 2697, 'MacHouse': 2698, 'stated': 2699, 'phonecalls': 2700, 'difficult': 2701, 'Micro': 2702, 'Center': 2703, 'his': 2704, 'adapter': 2705, 'isolated': 2706, 'incident': 2707, 'maker': 2708, 'intuition': 2709, 'beauty': 2710, 'technological': 2711, 'advances': 2712, 'associated': 2713, 'entertainment': 2714, 'telling': 2715, 'Crisp': 2716, 'sister': 2717, 'exact': 2718, 'split': 2719, 'everyday': 2720, 'Satellite': 2721, 'adjust': 2722, 'sensitivity': 2723, 'spongey': 2724, 'feeling': 2725, 'osx': 2726, 'calculator': 2727, 'Dummies': 2728, 'among': 2729, 'Paralles': 2730, 'virtual': 2731, 'Server': 2732, 'Enterprise': 2733, '2003': 2734, '2008': 2735, 'handles': 2736, 'vary': 2737, 'folks': 2738, 'indicate': 2739, 'intial': 2740}\n",
      "good 2 go\n",
      "good 2 go\n"
     ]
    }
   ],
   "source": [
    "mps_device = torch.device(\"mps\")\n",
    "\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "\n",
    "# Load training data\n",
    "training_data = []\n",
    "\n",
    "# getting training sentence tokens\n",
    "training_file_path = \"/Users/mo/Desktop/repos/nlp_a2/task_2/dataset/train_bio.json\"\n",
    "\n",
    "with open(training_file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    for entry in data.values():\n",
    "        sentence = entry[\"text\"].split()\n",
    "        tags = entry[\"labels\"]\n",
    "        training_data.append((sentence, tags))\n",
    "\n",
    "word_to_ix = {\"<UNK>\": 0}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "tag_to_ix = {\n",
    "    \"B\": 0,\n",
    "    \"I\": 1,\n",
    "    \"O\": 2,\n",
    "    START_TAG: 3,\n",
    "    STOP_TAG: 4\n",
    "}\n",
    "\n",
    "print(word_to_ix)\n",
    "\n",
    "#Load validation data\n",
    "validation_data = []\n",
    "\n",
    "# getting validation sentence tokens\n",
    "val_file_path = \"/Users/mo/Desktop/repos/nlp_a2/task_2/dataset/val_bio.json\"\n",
    "\n",
    "with open(val_file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    for entry in data.values():\n",
    "        sentence = entry[\"text\"].split()\n",
    "        tags = entry[\"labels\"]\n",
    "        validation_data.append((sentence, tags))\n",
    "\n",
    "\n",
    "#sanity check\n",
    "train_check = True\n",
    "for sentence, tags in training_data:\n",
    "    if (len(sentence) != len(tags)):\n",
    "        train_check = False\n",
    "        break\n",
    "\n",
    "if (train_check):\n",
    "    print(\"good 2 go\")\n",
    "else:\n",
    "    print(\"train prob\")\n",
    "\n",
    "val_check = True\n",
    "for sentence, tags in validation_data:\n",
    "    if (len(sentence) != len(tags)):\n",
    "        val_check = False\n",
    "        break\n",
    "\n",
    "if (val_check):\n",
    "    print(\"good 2 go\")\n",
    "else:\n",
    "    print(\"val prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 4\n",
    "\n",
    "# Load FastText embeddings from .vec file\n",
    "fasttext_model = KeyedVectors.load_word2vec_format('/Users/mo/Downloads/wiki-news-300d-1M-subword.vec')\n",
    "\n",
    "# Init torch embedding layer\n",
    "word_embeds = nn.Embedding(len(word_to_ix), EMBEDDING_DIM).to(mps_device)\n",
    "\n",
    "# Load FastText embeddings of size EMBEDDING_DIM into torch embedding layer\n",
    "for word, idx in word_to_ix.items():\n",
    "    if word in fasttext_model:\n",
    "        word_embeds.weight.data[idx] = torch.tensor(fasttext_model[word][:EMBEDDING_DIM], device=mps_device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "\n",
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM, word_embeds).to(mps_device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(12.8562, device='mps:0'), [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "#check predictions before training\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix).to(mps_device)\n",
    "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long).to(mps_device)\n",
    "    print(model(precheck_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epochs(model, optimizer, word_to_ix, tag_to_ix, train_data, val_data, run_name):\n",
    "\n",
    "    # train_loader, val_loader = get_data(tokenizer)\n",
    "\n",
    "    # Initialize W&B\n",
    "    wandb.login(key=\"7ef2e84866a68a6cd33c90b1fa55c8cf8ab2d6e7\", relogin=True)\n",
    "    wandb.init(project=\"nlp_a2\", name=\"BiLSTM_CRF_fasttext_t2\")\n",
    "    wandb.watch(model)\n",
    "\n",
    "    # Train and evaluate the model\n",
    "    num_epochs = 10\n",
    "    best_val_f1 = 0\n",
    "    best_val_loss = 100\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_f1 = 0\n",
    "\n",
    "        for sentence, tags in train_data:\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            sentence_in = prepare_sequence(sentence, word_to_ix).to(mps_device)\n",
    "            targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long).to(mps_device)\n",
    "\n",
    "            loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss\n",
    "\n",
    "            # Calculate F1-score\n",
    "            predicted = model(sentence_in)\n",
    "\n",
    "            targets = targets.cpu().numpy()\n",
    "\n",
    "            train_f1 += f1_score(targets, predicted[1], average=\"macro\")\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_f1 = 0\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for sentence, tags in val_data:\n",
    "                    \n",
    "                    sentence_in = prepare_sequence(sentence, word_to_ix).to(mps_device)\n",
    "                    targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long).to(mps_device)\n",
    "    \n",
    "                    loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "    \n",
    "                    val_loss += loss\n",
    "    \n",
    "                    # Calculate F1-score\n",
    "                    predicted = model(sentence_in)\n",
    "                    targets = targets.cpu().numpy()\n",
    "                    val_f1 += f1_score(targets, predicted[1], average=\"macro\")\n",
    "\n",
    "\n",
    "        # Log metrics to W&B\n",
    "        train_loss /= len(train_data)\n",
    "        val_loss /= len(val_data)\n",
    "        train_f1 /= len(train_data)\n",
    "        val_f1 /= len(val_data)\n",
    "\n",
    "        wandb.log(\n",
    "            {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"train_f1\": train_f1,\n",
    "                \"val_f1\": val_f1,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        # Check if training should be stopped\n",
    "        if epochs_without_improvement >= 3:\n",
    "            print(f\"Stopping early at epoch {epoch+1} due to no improvement.\")\n",
    "            break\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}, Train F1: {train_f1}, Val F1: {val_f1}\"\n",
    "        )\n",
    "\n",
    "    wandb.finish()\n",
    "    torch.save(model.state_dict(), run_name + \".pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/mo/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkaif21067\u001b[0m (\u001b[33mbigmeow\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/mo/Desktop/repos/nlp_a2/part_3/wandb/run-20240311_183454-fmk0jwpl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bigmeow/nlp_a2/runs/fmk0jwpl' target=\"_blank\">BiLSTM_CRF_fasttext_t2</a></strong> to <a href='https://wandb.ai/bigmeow/nlp_a2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bigmeow/nlp_a2' target=\"_blank\">https://wandb.ai/bigmeow/nlp_a2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bigmeow/nlp_a2/runs/fmk0jwpl' target=\"_blank\">https://wandb.ai/bigmeow/nlp_a2/runs/fmk0jwpl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 1/10 [04:27<40:11, 267.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: tensor([6.3614], device='mps:0', grad_fn=<DivBackward0>), Val Loss: tensor([4.7120], device='mps:0'), Train F1: 0.6227587885717877, Val F1: 0.6498382833790032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 2/10 [08:33<33:57, 254.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: tensor([3.7270], device='mps:0', grad_fn=<DivBackward0>), Val Loss: tensor([4.8503], device='mps:0'), Train F1: 0.7082883130830572, Val F1: 0.6632981456393727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  30%|███       | 3/10 [12:43<29:29, 252.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: tensor([3.1330], device='mps:0', grad_fn=<DivBackward0>), Val Loss: tensor([5.2668], device='mps:0'), Train F1: 0.7651903102347027, Val F1: 0.6786104702216371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  30%|███       | 3/10 [16:18<38:03, 326.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early at epoch 4 due to no improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a689d1da02042fea3afc6f6f78352d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>train_f1</td><td>▁▄▆█</td></tr><tr><td>train_loss</td><td>█▃▂▁</td></tr><tr><td>val_f1</td><td>▁▄▇█</td></tr><tr><td>val_loss</td><td>▁▂▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_f1</td><td>0.81638</td></tr><tr><td>train_loss</td><td>2.70271</td></tr><tr><td>val_f1</td><td>0.68216</td></tr><tr><td>val_loss</td><td>5.41003</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">BiLSTM_CRF_fasttext_t2</strong> at: <a href='https://wandb.ai/bigmeow/nlp_a2/runs/fmk0jwpl' target=\"_blank\">https://wandb.ai/bigmeow/nlp_a2/runs/fmk0jwpl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240311_183454-fmk0jwpl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_epochs(model, optimizer, word_to_ix, tag_to_ix, training_data, validation_data, \"BiLSTM_CRF_fasttext_t2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1: 0.65668631505872\n"
     ]
    }
   ],
   "source": [
    "# load test data\n",
    "\n",
    "test_data = []\n",
    "\n",
    "# getting test sentence tokens\n",
    "test_file_path = \"/Users/mo/Desktop/repos/nlp_a2/task_2/dataset/test_bio.json\"\n",
    "\n",
    "with open(test_file_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    for entry in data.values():\n",
    "        sentence = entry[\"text\"].split()\n",
    "        tags = entry[\"labels\"]\n",
    "        test_data.append((sentence, tags))\n",
    "\n",
    "# load the model\n",
    "model.load_state_dict(torch.load(\"BiLSTM_CRF_fasttext_t2.pt\"))\n",
    "\n",
    "# calculate f1 score\n",
    "\n",
    "test_f1 = 0\n",
    "with torch.no_grad():\n",
    "    for sentence, tags in test_data:\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix).to(mps_device)\n",
    "        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long).to(mps_device)\n",
    "\n",
    "        predicted = model(sentence_in)\n",
    "        targets = targets.cpu().numpy()\n",
    "\n",
    "        test_f1 += f1_score(targets, predicted[1], average=\"macro\")\n",
    "\n",
    "test_f1 /= len(test_data)\n",
    "print(f\"Test F1: {test_f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
